{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# import gensim\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "from scipy import sparse\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import time\n",
    "\n",
    "from attention import * \n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.rnn import GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 100 # this is the longest number of sentences in a document!!!\n",
    "MAX_NB_WORDS = 60000\n",
    "EMBEDDING_DIM = 300 #due to word2vec dimension!!! \n",
    "HIDDEN_SIZE = 50 #based on Yang et al CMU. (Hierachical Attention Networks for Document Classification)\n",
    "ATTENTION_SIZE = 100 #same as Yang et al. \n",
    "BATCH_SIZE = 50\n",
    "NUM_ITERS = 100\n",
    "DISPLAY_STEP = 10\n",
    "VALIDATION_STEP = 10\n",
    "TESTING_BATCH = 10\n",
    "USER_EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time of loading Word2Vec model:  100.92853212356567\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "# Download GoogleNews-vectors-negative300.bin.gz at \n",
    "#https://github.com/mmihaltz/word2vec-GoogleNews-vectors/blob/master/GoogleNews-vectors-negative300.bin.gz\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "toc = time.time()\n",
    "print(\"Running time of loading Word2Vec model: \", (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size):\n",
    "    \"\"\"Primitive batch generator \n",
    "    \"\"\"\n",
    "    size = X.shape[0]\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    X_copy = X_copy[indices]\n",
    "    y_copy = y_copy[indices]\n",
    "    i=0\n",
    "\n",
    "    while True:\n",
    "        left, right = i*batch_size, (i+1)*batch_size\n",
    "        right = min(size, right)\n",
    "        yield X_copy[left:right], y_copy[left:right]\n",
    "        if right >= size:\n",
    "            i = 0\n",
    "            indices = np.arange(size)\n",
    "            X_copy = X_copy[indices]\n",
    "            y_copy = y_copy[indices]\n",
    "        else:\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer_based_on_training(train_folder, test_folder, dict_selected_words,\n",
    "                                   ground_truth_file='snopes_ground_truth.csv'):\n",
    "    '''Return tensor data of URL based solely on training interactions only!!!'''\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(ground_truth_file)\n",
    "    full = zip(df['snopes_page'], df['claim_label'])\n",
    "    dict_url_ground_truth = {}\n",
    "    for url, label in full:\n",
    "        assert label == True or label == False\n",
    "        if label == True:\n",
    "            dict_url_ground_truth[url] = 1\n",
    "        elif label == False:\n",
    "            dict_url_ground_truth[url] = 0\n",
    "        \n",
    "    assert len(dict_url_ground_truth) == 562\n",
    "    \n",
    "    def read_text_files(infolder):\n",
    "        documents = [fn for fn in listdir(infolder) if fn.endswith('.txt')]\n",
    "        data = np.zeros((len(documents), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "        dict_docs = {}\n",
    "        for fn in documents:\n",
    "            p = join(infolder, fn)\n",
    "            fin1 = open(p, 'r')\n",
    "            url = fin1.readline().replace('\\n', '')\n",
    "            assert 'http' in url\n",
    "            sents = []\n",
    "            for line in fin1:\n",
    "                sents.append(line.replace('\\n', ''))\n",
    "            dict_docs[url] = sents\n",
    "                \n",
    "        Y = []\n",
    "        for idx, url in enumerate(dict_docs.keys()):\n",
    "            assert 'http' in url\n",
    "            label = dict_url_ground_truth[url]\n",
    "            assert label == 0 or label == 1\n",
    "            Y.append([label, 1-label])\n",
    "            sentences = dict_docs[url]\n",
    "            for j, sent in enumerate(sentences):\n",
    "                if j < MAX_SENTS:\n",
    "                    wordTokens = text_to_word_sequence(sent)\n",
    "                    k = 0\n",
    "                    for _, word in enumerate(wordTokens):\n",
    "                        if word not in dict_selected_words:\n",
    "                            continue\n",
    "                        index_of_word = dict_selected_words[word]\n",
    "                        assert index_of_word >= 1 and index_of_word <= 16000\n",
    "                        if k < MAX_SENT_LENGTH:\n",
    "                            data[idx, j, k] = index_of_word\n",
    "                            k+=1\n",
    "                    \n",
    "        return data, np.array(Y)\n",
    "        \n",
    "    X_train, y_train = read_text_files(infolder=train_folder)\n",
    "    X_test, y_test = read_text_files(infolder=test_folder)\n",
    "    print(y_train.shape, \"here\")\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_url_content_text2num(selected_words_file='out_top_16K_words_file.txt'):\n",
    "    parent = \"train_test_data\"\n",
    "    dict_folds = {}\n",
    "    fin = open(selected_words_file, 'r')\n",
    "    cnt = 1\n",
    "    dict_selected_words = {}\n",
    "    for line in fin:\n",
    "        _, w, _ = line.split()\n",
    "        dict_selected_words[w] = cnt\n",
    "        cnt += 1\n",
    "    assert len(dict_selected_words) == 16000 and max(dict_selected_words.values()) == 16000\n",
    "    fin.close()\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        #########################################################\n",
    "        dict_words = {} #for stat\n",
    "        data_i = '%s/data_%s' % (parent, i)\n",
    "        train_folder = '%s/train' % data_i\n",
    "        test_folder = '%s/test' % data_i\n",
    "        X_train, y_train, X_test, y_test = fit_tokenizer_based_on_training(train_folder, test_folder, dict_selected_words)\n",
    "        dict_folds[i] = (X_train, y_train, X_test, y_test)\n",
    "        \n",
    "    return dict_folds, dict_selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448, 2) here\n",
      "(448, 2) here\n",
      "(448, 2) here\n",
      "(448, 2) here\n",
      "(448, 2) here\n"
     ]
    }
   ],
   "source": [
    "dict_folds, dict_selected_words = load_url_content_text2num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vocabs = 16000 \n",
    "'''Word2vec embedding matrix'''\n",
    "embedding_matrix = np.random.random((no_vocabs, EMBEDDING_DIM))\n",
    "#i starts at 1 not 0 like normal stuff!!!!\n",
    "for word, i in dict_selected_words.items(): \n",
    "    if word in word2vec.wv.vocab:\n",
    "        embedding_vector = word2vec[word]\n",
    "        embedding_matrix[i-1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(true_labels, pred_labels):\n",
    "    assert y_true.shape == y_hat.shape\n",
    "    # True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "    TP = np.sum(np.logical_and(pred_labels == 1, true_labels == 1))  \n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(pred_labels == 0, true_labels == 0))  \n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(pred_labels == 1, true_labels == 0))  \n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(pred_labels == 0, true_labels == 1))  \n",
    "    #print 'TP: %i, FP: %i, TN: %i, FN: %i' % (TP,FP,TN,FN)\n",
    "    assert TP+ TN+ FP+ FN == len(true_labels)\n",
    "    return TP, TN, FP, FN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([0,0, 1, 1, 1, 0, 0, 0, 1, 0]) \n",
    "y_hat = np.array([0, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "print(computeMetrics(y_true, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamBiRNN(X_train, y_train, X_test, y_test, log_dir, outfile):\n",
    "        tf.reset_default_graph() \n",
    "        train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "        fout = open(outfile, 'w')\n",
    "        '''Variable with shape (no_vocabs, EMBEDDING_DIM) to get vectors in a sentence'''\n",
    "        embedding_matrix_variable = tf.Variable(embedding_matrix, trainable=True, dtype=tf.float32)\n",
    "        #print(embedding_matrix_variable.shape)\n",
    "        #print(X_train.shape)\n",
    "\n",
    "        '''We will take a bunch of sentences, where each sentence has length MAX_SENT_LENGTH\n",
    "        Ex: Two sentences: [[1,2,5,6,0], [3,5,3,6,0]] where numbers indicate a word. We will look up the \n",
    "        word vector for each word based on the number. \n",
    "        '''\n",
    "        #batch_sent_ph = tf.placeholder(tf.int32, [None, MAX_SENT_LENGTH]) \n",
    "        '''\n",
    "        Hope it work. After looking up, the shape should be \n",
    "        (batch_size, MAX_SENTS, MAX_SENT_LENGTH, EMBEDDING_DIM)\n",
    "        However, since we need to use bi_rnn and we learn representation of sentences first. \n",
    "        Therefore, we should use shape \n",
    "        (?, MAX_SENT_LENGTH, EMBEDDING_DIM) where \"?\" should be a multiple of MAX_SENTS\n",
    "        '''\n",
    "        batch_sent_ph = tf.placeholder(tf.int32, [None, MAX_SENT_LENGTH], name=\"batch_sent_ph\")\n",
    "        batch_sent_embedded = tf.nn.embedding_lookup(embedding_matrix_variable, batch_sent_ph)\n",
    "        y_ph = tf.placeholder(tf.float32, [None, 2], name=\"labels\")\n",
    "        sentence_length_ph = tf.placeholder(tf.int32, [None], name=\"sentence_length_ph\")\n",
    "        doc_actual_length_ph = tf.placeholder(tf.int32, [None], name=\"doc_actual_length_ph\")\n",
    "        #print(batch_sent_embedded)\n",
    "\n",
    "        '''We do not specify sequence_length. \n",
    "        Therefore, the number of GRU cell in forward (same as backward) is MAX_SENT_LENGTH'''\n",
    "        with tf.variable_scope(\"first_bi_rnn\"):\n",
    "            rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE), \n",
    "                                    inputs=batch_sent_embedded, \n",
    "                                    sequence_length=sentence_length_ph, \n",
    "                                    dtype=tf.float32)\n",
    "        with tf.name_scope(\"attention_first_bi_rnn\"):\n",
    "            attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "        with tf.name_scope(\"dropout_afterfirst_bi_rnn\"):\n",
    "            dropout_first_bi_rnn = tf.nn.dropout(attention_output, keep_prob=0.8)\n",
    "        with tf.name_scope(\"sent_bedding_after_first_birnn\"):\n",
    "            sent_bedding_after_first_birnn = tf.reshape(dropout_first_bi_rnn, shape=[-1, MAX_SENTS, 2*HIDDEN_SIZE])\n",
    "        ###########second bi-rnn-layer ############################\n",
    "        with tf.variable_scope(\"second_bi_rnn\"):\n",
    "            bi_rnn_sent_outputs, _ = bi_rnn(GRUCell(2*HIDDEN_SIZE), GRUCell(2*HIDDEN_SIZE), \n",
    "                                            inputs=sent_bedding_after_first_birnn, \n",
    "                                            sequence_length=doc_actual_length_ph,\n",
    "                                            dtype=tf.float32)\n",
    "        with tf.name_scope(\"attention_second_bi_rnn\"):\n",
    "            attention_output2, alphas = attention(bi_rnn_sent_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "        with tf.name_scope(\"dropout_after_second_bi_rnn\"):\n",
    "            dropout_second_bi_rnn = tf.nn.dropout(attention_output2, keep_prob=0.8)\n",
    "\n",
    "        with tf.name_scope(\"FC_layer\"):\n",
    "            W = tf.Variable(tf.random_normal([HIDDEN_SIZE * 4, 2], stddev=0.1))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "            y_hat = tf.matmul(dropout_second_bi_rnn, W) + b\n",
    "        #y_hat = tf.squeeze(y_hat)\n",
    "\n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            out_softmax = tf.nn.softmax(logits=y_hat)\n",
    "\n",
    "        with tf.name_scope(\"loss_cross_entropy\"):\n",
    "            loss = -tf.reduce_mean(tf.reduce_sum(tf.cast(y_ph, tf.float32) * tf.log(out_softmax), axis=1))\n",
    "        #loss = tf.reduce_sum(out_softmax)\n",
    "        A = tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "        with tf.variable_scope(\"Traininig\"):\n",
    "            train_step = tf.train.AdamOptimizer(1e-4).minimize(loss=loss)\n",
    "\n",
    "        with tf.variable_scope(\"evaluation\"):\n",
    "            ground_truth = tf.argmax(y_ph, 1)\n",
    "            predicted = tf.argmax(out_softmax, 1)\n",
    "            correct_prediction = tf.equal(predicted, ground_truth)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            accuracy_test = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        valVar = tf.Variable(0.0, \"valVar\")\n",
    "        valVal_ph = tf.placeholder(tf.float32, [], name=\"independent\")\n",
    "        update_valVar = valVar.assign(valVal_ph)\n",
    "        mySummary = tf.summary.scalar(\"Validation\", update_valVar)\n",
    "\n",
    "        B = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "        summary_op = tf.summary.merge([A, B])\n",
    "        #summary_op = tf.summary.merge_all()\n",
    "\n",
    "        #### Testing model phat ##########################\n",
    "        #TODO\n",
    "        pre_val_acc = -1\n",
    "        best_results = None\n",
    "        time_val_acc_reduced = 0\n",
    "\n",
    "        placeholder_input = (batch_sent_ph, y_ph, sentence_length_ph, doc_actual_length_ph)\n",
    "        STOP_TRAINING = False\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            saver.save(sess, 'saved_models/my_test_model')\n",
    "\n",
    "            writer = tf.summary.FileWriter(log_dir, graph=sess.graph)\n",
    "            cnt_step = 0\n",
    "            for i in range(NUM_ITERS):\n",
    "                num_batches = X_train.shape[0] / float(BATCH_SIZE)\n",
    "                num_batches = int(num_batches)\n",
    "                #num_batches = 11\n",
    "                if STOP_TRAINING:\n",
    "                    break\n",
    "                for b in range(num_batches):\n",
    "                    if STOP_TRAINING:\n",
    "                        break\n",
    "                    x_batch, y_batch = next(train_batch_generator)\n",
    "                    temp = np.sum(x_batch, axis=2)\n",
    "                    doc_actual_lengths_better = np.count_nonzero(temp, axis=1) \n",
    "\n",
    "                    #when reshaping data for feeddict, you should use np.reshape\n",
    "                    x_batch = x_batch.reshape(-1, MAX_SENT_LENGTH)\n",
    "                    '''Actual length of sentences in this batch_size * so_luong_sentence_moi_doc'''\n",
    "                    sentence_actual_lengths_better = np.count_nonzero(x_batch, axis=1)\n",
    "                    if cnt_step % DISPLAY_STEP == 0:\n",
    "                        #print(\"At iter %s and batch %s of %s - cntStep: %s\" % (i, b, num_batches, cnt_step))\n",
    "                        train_acc = accuracy.eval(feed_dict={batch_sent_ph: x_batch, y_ph: y_batch,\n",
    "                                                           sentence_length_ph: sentence_actual_lengths_better,\n",
    "                                                           doc_actual_length_ph: doc_actual_lengths_better\n",
    "                                                           })\n",
    "                        print(\"Training accuracy at [iter %s][batch %s of %s][cntStep: %s] : %s\" % \n",
    "                              (i, b, num_batches, cnt_step, train_acc))\n",
    "                        fout.write('%s\\n' % \"Training accuracy at [iter %s][batch %s of %s][cntStep: %s] : %s\" % \n",
    "                              (i, b, num_batches, cnt_step, train_acc))\n",
    "                        fout.flush()\n",
    "                    ####################################################\n",
    "                    ### VALIDATION STEP TO AVOID OVERFITTING!!!!\n",
    "                    ########################################################\n",
    "                    if cnt_step % VALIDATION_STEP == 0:\n",
    "                        #do validation\n",
    "                        curr_val_acc, curr_fp, curr_fn = doValidation(X_data=X_test, y_data=y_test, curr_sess=sess,\n",
    "                                     metrics=[accuracy_test, ground_truth, predicted], \n",
    "                                                    batch_size=TESTING_BATCH, prefix=\"Validation\",\n",
    "                                    placeholder_input=placeholder_input)\n",
    "                        print(('Validation accuracy is: %s, fp:%s, fn:%s on shape %s' % \n",
    "                                             (curr_val_acc, curr_fp, curr_fn, str(X_test.shape))))\n",
    "                        fout.write('%s\\n' % ('Validation accuracy is: %s, fp:%s, fn:%s on shape %s' % \n",
    "                                             (curr_val_acc, curr_fp, curr_fn, str(X_test.shape))))\n",
    "                        fout.flush()\n",
    "                        _, ss = sess.run([update_valVar, mySummary], feed_dict={valVal_ph: curr_val_acc})\n",
    "                        writer.add_summary(ss, cnt_step)\n",
    "\n",
    "                        if curr_val_acc >= pre_val_acc:\n",
    "                            pre_val_acc = curr_val_acc\n",
    "                            time_val_acc_reduced = 0\n",
    "                            best_results = [curr_val_acc, curr_fp, curr_fn]\n",
    "#                         else:\n",
    "#                             time_val_acc_reduced +=1\n",
    "#                             if time_val_acc_reduced >= 10:\n",
    "#                                 #10 times accuracy reduced over time, we stop training!!!!\n",
    "#                                 print(\"Validation reduced!!! We should stop training here!!!!\")\n",
    "#                                 STOP_TRAINING = True\n",
    "\n",
    "                    #assert x_batch.shape == (BATCH_SIZE * MAX_SENTS, MAX_SENT_LENGTH), x_batch.shape\n",
    "                    summary, _ = sess.run([summary_op, train_step], \n",
    "                                                 feed_dict={batch_sent_ph: x_batch, y_ph: y_batch,\n",
    "                                                           sentence_length_ph: sentence_actual_lengths_better,\n",
    "                                                           doc_actual_length_ph: doc_actual_lengths_better\n",
    "                                                           })\n",
    "                    writer.add_summary(summary, cnt_step)\n",
    "                    cnt_step += 1\n",
    "\n",
    "            writer.close()\n",
    "            #testing data\n",
    "            best_testing_acc = pre_val_acc\n",
    "            print('Best Testing accuracy is: ', best_testing_acc, ' on shape ', X_test.shape)\n",
    "            fout.write('%s\\n' % ('Best Testing ACC: %s, FP:%s, FN:%s on shape %s' % (best_results[0], best_results[1], \n",
    "                                                                                     best_results[2], str(X_test.shape))))\n",
    "            fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doValidation(X_data, y_data, curr_sess, metrics, batch_size, prefix, placeholder_input):\n",
    "        accuracy, ground_truth, predicted = metrics\n",
    "        num_batches = X_data.shape[0] / float(batch_size)\n",
    "        num_batches = int(num_batches)\n",
    "        sum_acc, sum_fp, sum_fn, sum_TP, sum_TN = 0, 0, 0, 0, 0\n",
    "        cnt_rows = 0\n",
    "        batch_sent_ph, y_ph, sentence_length_ph, doc_actual_length_ph = placeholder_input\n",
    "        test_batch_generator = batch_generator(X_data, y_data, batch_size)\n",
    "        for i in range(num_batches+1):\n",
    "\n",
    "            x_batch, y_batch = next(test_batch_generator)\n",
    "            temp = np.sum(x_batch, axis=2)\n",
    "            #print(\"Test data shape: \", x_batch.shape)\n",
    "            doc_actual_lengths_test = np.count_nonzero(temp, axis=1) \n",
    "            x_batch_reshaped = x_batch.reshape(-1, MAX_SENT_LENGTH)\n",
    "            sentence_actual_lengths_test = np.count_nonzero(x_batch_reshaped, axis=1)\n",
    "            acc_result, true_labels, pred_labels = curr_sess.run([accuracy, ground_truth, predicted], \n",
    "                                                       feed_dict={batch_sent_ph: x_batch_reshaped, \n",
    "                                        y_ph: y_batch, sentence_length_ph: sentence_actual_lengths_test,\n",
    "                                        doc_actual_length_ph: doc_actual_lengths_test})\n",
    "            \n",
    "            tp_res, tn_res, fp_res, fn_res = computeMetrics(true_labels=true_labels, pred_labels=pred_labels)\n",
    "\n",
    "            assert abs(fp_res+tn_res+tp_res+fn_res - x_batch.shape[0]) < 1e-10, 'fp: %s, tn: %s, tp: %s, fn:%s vs. %s' % (fp_res, tn_res, tp_res, fn_res, x_batch.shape[0])\n",
    "            sum_acc += acc_result\n",
    "            sum_fp += fp_res\n",
    "            sum_fn += fn_res\n",
    "            sum_TP += tp_res\n",
    "            sum_TN += tn_res\n",
    "            #sum_fp += (fp_res/float(fp_res+tn_res))\n",
    "            #sum_fn += fn_res/float(tp_res+fn_res)\n",
    "            cnt_rows += x_batch.shape[0]\n",
    "        assert cnt_rows == X_data.shape[0], 'Mismatched rows_count: %s vs. %s' % (cnt_rows, X_data.shape[0])\n",
    "        rr = sum_acc/float(X_data.shape[0])\n",
    "        rr_fp = sum_fp/float(sum_fp+ sum_TN)\n",
    "        rr_fn = sum_fn/float(sum_TP+ sum_fn)\n",
    "        return rr, rr_fp, rr_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BiGRU():\n",
    "    log_dir = \"HAM_log\"\n",
    "    \n",
    "\n",
    "    for key in dict_folds:\n",
    "        X_train, y_train, X_test, y_test = dict_folds[key]\n",
    "#         train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "#         test_batch_generator = batch_generator(X_test, y_test, TESTING_BATCH)\n",
    "\n",
    "        #clear log_dir\n",
    "        if os.path.exists(log_dir):\n",
    "            shutil.rmtree(log_dir)\n",
    "\n",
    "        hamBiRNN(X_train, y_train, X_test, y_test, log_dir, outfile='log_result_%s' % (key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy at [iter 0][batch 0 of 8][cntStep: 0] : 0.48\n",
      "Validation accuracy is: 0.561403508772, fp:0.210526315789, fn:0.666666666667 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 1][batch 2 of 8][cntStep: 10] : 0.6\n",
      "Validation accuracy is: 0.614035087719, fp:0.157894736842, fn:0.614035087719 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 2][batch 4 of 8][cntStep: 20] : 0.36\n",
      "Validation accuracy is: 0.526315789474, fp:0.0, fn:0.947368421053 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 3][batch 6 of 8][cntStep: 30] : 0.58\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 5][batch 0 of 8][cntStep: 40] : 0.38\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 6][batch 2 of 8][cntStep: 50] : 0.46\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 7][batch 4 of 8][cntStep: 60] : 0.7\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 8][batch 6 of 8][cntStep: 70] : 0.42\n",
      "Validation accuracy is: 0.491228070175, fp:0.0175438596491, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 10][batch 0 of 8][cntStep: 80] : 0.604167\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 11][batch 2 of 8][cntStep: 90] : 0.52\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 12][batch 4 of 8][cntStep: 100] : 0.5\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 13][batch 6 of 8][cntStep: 110] : 0.34\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 15][batch 0 of 8][cntStep: 120] : 0.58\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 16][batch 2 of 8][cntStep: 130] : 0.38\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 17][batch 4 of 8][cntStep: 140] : 0.46\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 18][batch 6 of 8][cntStep: 150] : 0.7\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 20][batch 0 of 8][cntStep: 160] : 0.42\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 21][batch 2 of 8][cntStep: 170] : 0.604167\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 22][batch 4 of 8][cntStep: 180] : 0.52\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 23][batch 6 of 8][cntStep: 190] : 0.52\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 25][batch 0 of 8][cntStep: 200] : 0.48\n",
      "Validation accuracy is: 0.5, fp:0.0175438596491, fn:0.982456140351 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 26][batch 2 of 8][cntStep: 210] : 0.88\n",
      "Validation accuracy is: 0.517543859649, fp:0.245614035088, fn:0.719298245614 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 27][batch 4 of 8][cntStep: 220] : 0.78\n",
      "Validation accuracy is: 0.570175438596, fp:0.0526315789474, fn:0.80701754386 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 28][batch 6 of 8][cntStep: 230] : 0.84\n",
      "Validation accuracy is: 0.614035087719, fp:0.19298245614, fn:0.578947368421 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 30][batch 0 of 8][cntStep: 240] : 1.0\n",
      "Validation accuracy is: 0.657894736842, fp:0.350877192982, fn:0.333333333333 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 31][batch 2 of 8][cntStep: 250] : 0.98\n",
      "Validation accuracy is: 0.710526315789, fp:0.280701754386, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 32][batch 4 of 8][cntStep: 260] : 0.958333\n",
      "Validation accuracy is: 0.754385964912, fp:0.245614035088, fn:0.245614035088 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 33][batch 6 of 8][cntStep: 270] : 0.98\n",
      "Validation accuracy is: 0.754385964912, fp:0.175438596491, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 35][batch 0 of 8][cntStep: 280] : 1.0\n",
      "Validation accuracy is: 0.701754385965, fp:0.438596491228, fn:0.157894736842 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 36][batch 2 of 8][cntStep: 290] : 1.0\n",
      "Validation accuracy is: 0.719298245614, fp:0.140350877193, fn:0.421052631579 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 37][batch 4 of 8][cntStep: 300] : 1.0\n",
      "Validation accuracy is: 0.710526315789, fp:0.350877192982, fn:0.228070175439 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 38][batch 6 of 8][cntStep: 310] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.140350877193, fn:0.40350877193 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 40][batch 0 of 8][cntStep: 320] : 1.0\n",
      "Validation accuracy is: 0.684210526316, fp:0.508771929825, fn:0.122807017544 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 41][batch 2 of 8][cntStep: 330] : 1.0\n",
      "Validation accuracy is: 0.675438596491, fp:0.473684210526, fn:0.175438596491 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 42][batch 4 of 8][cntStep: 340] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.350877192982, fn:0.175438596491 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 43][batch 6 of 8][cntStep: 350] : 1.0\n",
      "Validation accuracy is: 0.701754385965, fp:0.298245614035, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 45][batch 0 of 8][cntStep: 360] : 1.0\n",
      "Validation accuracy is: 0.701754385965, fp:0.245614035088, fn:0.350877192982 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 46][batch 2 of 8][cntStep: 370] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.228070175439, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 47][batch 4 of 8][cntStep: 380] : 1.0\n",
      "Validation accuracy is: 0.719298245614, fp:0.245614035088, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 48][batch 6 of 8][cntStep: 390] : 1.0\n",
      "Validation accuracy is: 0.701754385965, fp:0.280701754386, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 50][batch 0 of 8][cntStep: 400] : 1.0\n",
      "Validation accuracy is: 0.710526315789, fp:0.280701754386, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 51][batch 2 of 8][cntStep: 410] : 1.0\n",
      "Validation accuracy is: 0.763157894737, fp:0.210526315789, fn:0.263157894737 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 52][batch 4 of 8][cntStep: 420] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.210526315789, fn:0.333333333333 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 53][batch 6 of 8][cntStep: 430] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.333333333333, fn:0.19298245614 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 55][batch 0 of 8][cntStep: 440] : 0.979167\n",
      "Validation accuracy is: 0.69298245614, fp:0.508771929825, fn:0.105263157895 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 56][batch 2 of 8][cntStep: 450] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.350877192982, fn:0.19298245614 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 57][batch 4 of 8][cntStep: 460] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.157894736842, fn:0.368421052632 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 58][batch 6 of 8][cntStep: 470] : 1.0\n",
      "Validation accuracy is: 0.763157894737, fp:0.122807017544, fn:0.350877192982 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 60][batch 0 of 8][cntStep: 480] : 1.0\n",
      "Validation accuracy is: 0.754385964912, fp:0.175438596491, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 61][batch 2 of 8][cntStep: 490] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.245614035088, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 62][batch 4 of 8][cntStep: 500] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.210526315789, fn:0.333333333333 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 63][batch 6 of 8][cntStep: 510] : 1.0\n",
      "Validation accuracy is: 0.719298245614, fp:0.228070175439, fn:0.333333333333 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 65][batch 0 of 8][cntStep: 520] : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.728070175439, fp:0.210526315789, fn:0.333333333333 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 66][batch 2 of 8][cntStep: 530] : 1.0\n",
      "Validation accuracy is: 0.754385964912, fp:0.210526315789, fn:0.280701754386 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 67][batch 4 of 8][cntStep: 540] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.210526315789, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 68][batch 6 of 8][cntStep: 550] : 1.0\n",
      "Validation accuracy is: 0.745614035088, fp:0.19298245614, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 70][batch 0 of 8][cntStep: 560] : 1.0\n",
      "Validation accuracy is: 0.763157894737, fp:0.19298245614, fn:0.280701754386 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 71][batch 2 of 8][cntStep: 570] : 1.0\n",
      "Validation accuracy is: 0.745614035088, fp:0.210526315789, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 72][batch 4 of 8][cntStep: 580] : 1.0\n",
      "Validation accuracy is: 0.701754385965, fp:0.245614035088, fn:0.350877192982 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 73][batch 6 of 8][cntStep: 590] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.228070175439, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 75][batch 0 of 8][cntStep: 600] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.157894736842, fn:0.368421052632 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 76][batch 2 of 8][cntStep: 610] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.210526315789, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 77][batch 4 of 8][cntStep: 620] : 1.0\n",
      "Validation accuracy is: 0.745614035088, fp:0.19298245614, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 78][batch 6 of 8][cntStep: 630] : 1.0\n",
      "Validation accuracy is: 0.745614035088, fp:0.19298245614, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 80][batch 0 of 8][cntStep: 640] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.19298245614, fn:0.350877192982 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 81][batch 2 of 8][cntStep: 650] : 1.0\n",
      "Validation accuracy is: 0.763157894737, fp:0.175438596491, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 82][batch 4 of 8][cntStep: 660] : 1.0\n",
      "Validation accuracy is: 0.719298245614, fp:0.245614035088, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 83][batch 6 of 8][cntStep: 670] : 1.0\n",
      "Validation accuracy is: 0.728070175439, fp:0.245614035088, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 85][batch 0 of 8][cntStep: 680] : 1.0\n",
      "Validation accuracy is: 0.745614035088, fp:0.210526315789, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 86][batch 2 of 8][cntStep: 690] : 1.0\n",
      "Validation accuracy is: 0.710526315789, fp:0.228070175439, fn:0.350877192982 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 87][batch 4 of 8][cntStep: 700] : 1.0\n",
      "Validation accuracy is: 0.736842105263, fp:0.175438596491, fn:0.350877192982 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 88][batch 6 of 8][cntStep: 710] : 1.0\n",
      "Validation accuracy is: 0.745614035088, fp:0.19298245614, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 90][batch 0 of 8][cntStep: 720] : 1.0\n",
      "Validation accuracy is: 0.719298245614, fp:0.210526315789, fn:0.350877192982 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 91][batch 2 of 8][cntStep: 730] : 1.0\n",
      "Validation accuracy is: 0.754385964912, fp:0.157894736842, fn:0.333333333333 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 92][batch 4 of 8][cntStep: 740] : 1.0\n",
      "Validation accuracy is: 0.754385964912, fp:0.175438596491, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 93][batch 6 of 8][cntStep: 750] : 1.0\n",
      "Validation accuracy is: 0.763157894737, fp:0.175438596491, fn:0.298245614035 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 95][batch 0 of 8][cntStep: 760] : 1.0\n",
      "Validation accuracy is: 0.771929824561, fp:0.140350877193, fn:0.315789473684 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 96][batch 2 of 8][cntStep: 770] : 1.0\n",
      "Validation accuracy is: 0.684210526316, fp:0.508771929825, fn:0.122807017544 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 97][batch 4 of 8][cntStep: 780] : 1.0\n",
      "Validation accuracy is: 0.684210526316, fp:0.245614035088, fn:0.385964912281 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 98][batch 6 of 8][cntStep: 790] : 1.0\n",
      "Validation accuracy is: 0.710526315789, fp:0.0526315789474, fn:0.526315789474 on shape (114, 100, 100)\n",
      "Best Testing accuracy is:  0.771929824561  on shape  (114, 100, 100)\n",
      "Training accuracy at [iter 0][batch 0 of 8][cntStep: 0] : 0.54\n",
      "Validation accuracy is: 0.491228070175, fp:0.842105263158, fn:0.175438596491 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 1][batch 2 of 8][cntStep: 10] : 0.54\n",
      "Validation accuracy is: 0.5, fp:1.0, fn:0.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 2][batch 4 of 8][cntStep: 20] : 0.56\n",
      "Validation accuracy is: 0.561403508772, fp:0.40350877193, fn:0.473684210526 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 3][batch 6 of 8][cntStep: 30] : 0.54\n",
      "Validation accuracy is: 0.517543859649, fp:0.0350877192982, fn:0.929824561404 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 5][batch 0 of 8][cntStep: 40] : 0.6\n",
      "Validation accuracy is: 0.526315789474, fp:0.0, fn:0.947368421053 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 6][batch 2 of 8][cntStep: 50] : 0.5\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 7][batch 4 of 8][cntStep: 60] : 0.48\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 8][batch 6 of 8][cntStep: 70] : 0.52\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 10][batch 0 of 8][cntStep: 80] : 0.5625\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 11][batch 2 of 8][cntStep: 90] : 0.4\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n",
      "Training accuracy at [iter 12][batch 4 of 8][cntStep: 100] : 0.46\n",
      "Validation accuracy is: 0.5, fp:0.0, fn:1.0 on shape (114, 100, 100)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5d7943209fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_BiGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-28feed94cd45>\u001b[0m in \u001b[0;36mtrain_BiGRU\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mhamBiRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-53ae5aebb665>\u001b[0m in \u001b[0;36mhamBiRNN\u001b[0;34m(X_train, y_train, X_test, y_test, log_dir)\u001b[0m\n\u001b[1;32m    160\u001b[0m                                                  feed_dict={batch_sent_ph: x_batch, y_ph: y_batch,\n\u001b[1;32m    161\u001b[0m                                                            \u001b[0msentence_length_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence_actual_lengths_better\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                                            \u001b[0mdoc_actual_length_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_actual_lengths_better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                                                            })\n\u001b[1;32m    164\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_BiGRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
